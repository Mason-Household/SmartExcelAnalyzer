{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767754d9-b735-43ad-8709-58d10b792e1e",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client import QdrantClient\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class EnvironmentVariables(str, Enum):\n",
    "    QDRANT_HOST = \"QDRANT_HOST\"\n",
    "    QDRANT_PORT = \"QDRANT_PORT\"\n",
    "    QDRANT_API_KEY = \"QDRANT_API_KEY\"\n",
    "    QDRANT_USE_HTTPS = \"QDRANT_USE_HTTPS\"\n",
    "    EMBEDDING_MODEL = \"EMBEDDING_MODEL\"\n",
    "    TEXT_GENERATION_MODEL = \"TEXT_GENERATION_MODEL\"\n",
    "\n",
    "class Query(BaseModel):\n",
    "    document_id: str\n",
    "    question: str\n",
    "\n",
    "class QueryResponse(Query): \n",
    "    answer: str\n",
    "    relevantRows: list\n",
    "\n",
    "class ComputeEmbedding(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class ComputeBatchEmbeddings(BaseModel):\n",
    "    texts: list[str]\n",
    "\n",
    "default_qdrant_port = 6333\n",
    "default_qdrant_host = \"localhost\"\n",
    "default_text_generation_model = \"facebook/bart-large-cnn\"\n",
    "default_embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "QDRANT_HOST = os.getenv(EnvironmentVariables.QDRANT_HOST.value, default_qdrant_host)\n",
    "QDRANT_PORT = int(os.getenv(EnvironmentVariables.QDRANT_PORT.value, default_qdrant_port))\n",
    "QDRANT_USE_HTTPS = os.getenv(EnvironmentVariables.QDRANT_USE_HTTPS.value, \"false\").lower() == \"true\"\n",
    "EMBEDDING_MODEL = os.getenv(EnvironmentVariables.EMBEDDING_MODEL.value, default_embedding_model)\n",
    "TEXT_GENERATION_MODEL = os.getenv(EnvironmentVariables.TEXT_GENERATION_MODEL.value, default_text_generation_model)\n",
    "QDRANT_API_KEY = os.getenv(EnvironmentVariables.QDRANT_API_KEY.value)\n",
    "if not QDRANT_API_KEY:\n",
    "    raise ValueError(\"QDRANT_API_KEY is not set in the environment variables\")\n",
    "\n",
    "app = FastAPI()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "embedding_model = AutoModel.from_pretrained(EMBEDDING_MODEL)\n",
    "model = pipeline(\"text2text-generation\", model=TEXT_GENERATION_MODEL)\n",
    "qdrant_client = QdrantClient(\n",
    "    host=QDRANT_HOST,\n",
    "    port=QDRANT_PORT,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    prefer_grpc=False,\n",
    "    https=QDRANT_USE_HTTPS\n",
    ")\n",
    "\n",
    "@app.get(\"/health\", response_model=dict)\n",
    "async def health():\n",
    "    try:\n",
    "        print(\"Performing health check...\")\n",
    "        AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "        AutoModel.from_pretrained(EMBEDDING_MODEL)\n",
    "        pipeline(\"text2text-generation\", model=TEXT_GENERATION_MODEL)\n",
    "        qdrant_client.get_collections()  # Check Qdrant connection\n",
    "        print(\"Health check passed: LLM Service Endpoint, Qdrant, and LLM Models are all accessible.\")\n",
    "        return {\"status\": \"ok\"}\n",
    "    except Exception as e:\n",
    "        print(\"Health check failed.\")\n",
    "        print(e)\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/query\", response_model=QueryResponse)\n",
    "async def process_query(query: Query) -> QueryResponse:\n",
    "    try:\n",
    "        inputs = tokenizer(query.question, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            question_embedding = embedding_model(**inputs).last_hidden_state.mean(dim=1).numpy().tolist()[0]\n",
    "\n",
    "        logger.info(f\"Question Embedding: {question_embedding}\")\n",
    "\n",
    "        search_result = qdrant_client.search(\n",
    "            collection_name=\"documents\",\n",
    "            query_vector=question_embedding,\n",
    "            query_filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(\n",
    "                        key=\"document_id\",\n",
    "                        match=models.MatchValue(value=query.document_id)\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            limit=10\n",
    "        )\n",
    "        relevant_rows = [json.loads(hit.payload[\"content\"]) for hit in search_result]\n",
    "\n",
    "        logger.info(f\"Relevant Rows: {relevant_rows}\")\n",
    "\n",
    "        context = \" \".join([row[\"content\"] for row in relevant_rows])\n",
    "        prompt = f\"Given the following context: {context} Question: {query.question} Answer:\"\n",
    "        result = model(prompt, max_length=250, do_sample=False)[0]['generated_text']\n",
    "        return QueryResponse(\n",
    "            answer=result,\n",
    "            question=query.question,\n",
    "            document_id=query.document_id,\n",
    "            relevant_rows=relevant_rows\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query: {str(e)}\", exc_info=True)\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/compute_embedding\", response_model=list[float])\n",
    "async def compute_embedding(compute_embedding: ComputeEmbedding):\n",
    "    try:\n",
    "        inputs = tokenizer(compute_embedding.text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        return embeddings.numpy().tolist()[0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "@app.post(\"/compute_batch_embedding\", response_model=List[List[float]])\n",
    "async def compute_batch_embedding(compute_embedding: ComputeBatchEmbeddings):\n",
    "    try:\n",
    "        inputs = tokenizer(compute_embedding.texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.tolist()\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "async def general_exception_handler(request, exc):\n",
    "    logger.error(f\"An error occurred: {str(exc)}\", exc_info=True)\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\"message\": \"An internal error occurred\", \"detail\": str(exc)}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
